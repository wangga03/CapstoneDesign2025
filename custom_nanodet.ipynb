{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangga03/CapstoneDesign2025/blob/main/custom_nanodet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pOL8r1gPEW4"
      },
      "source": [
        "# Train NanoDet with custom dataset\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SonySemiconductorSolutions/aitrios-rpi-tutorials-ai-model-training/blob/main/notebooks/nanodet-ppe/custom_nanodet.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Training NanoDet model to detect Personal Protection Equipment (PPE) using open source dataset.\n",
        "\n",
        "Observe, that this tutorial requires GPU when used with Colab. Enable GPU:\n",
        "\n",
        "* Navigate to Editâ†’Notebook Settings\n",
        "* Select T4 GPU from Hardware Accelerator\n",
        "\n",
        "Nanodet training based on https://github.com/RangiLyu/nanodet/tree/main\n",
        "\n",
        "Tutorial includes:\n",
        "- Dataset setup\n",
        "- Nanodet model setup\n",
        "- Training\n",
        "- Quantization using [Model Compression Toolkit - MCT](https://github.com/sony/model_optimization)\n",
        "- COCO evaluation\n",
        "- Visualization\n",
        "- Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCmomek_PEW9"
      },
      "outputs": [],
      "source": [
        "!pip install -q --no-cache-dir torch~=2.5.0 torchvision tensorflow~=2.14.0 pycocotools imx500-converter[tf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSWNVX7gPEW-"
      },
      "outputs": [],
      "source": [
        "# Converter requires java\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "def install_java(package: str = 'openjdk-17-jdk', version: int = 17) -> bool:\n",
        "    try:\n",
        "        result = subprocess.run(['java', '--version'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        version_output = result.stdout.splitlines()[0]\n",
        "        match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', version_output)  # Match version in form major.minor.patch\n",
        "        print(f\"Found Java version: {match.group(0)}\")\n",
        "        if match:\n",
        "            major_version = int(match.group(1))\n",
        "            if major_version == version:\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Java {version} is not installed. Installing correct version...\")\n",
        "    except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
        "        print(f\"Java not installed. Installing...\")\n",
        "\n",
        "    try:\n",
        "        is_root = os.geteuid() == 0\n",
        "        prefix = [] if is_root else ['sudo']\n",
        "        with open(os.devnull, 'w') as devnull:\n",
        "            subprocess.run(prefix + ['apt', 'install', '-y', package], check=True, stdout=devnull, stderr=devnull)\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Installation error: {e}\")\n",
        "        return False\n",
        "\n",
        "if install_java():\n",
        "    print(f'Java installed')\n",
        "else:\n",
        "    print(f'Java missing and installation failed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G6co0zTPEW_"
      },
      "outputs": [],
      "source": [
        "# Perform initial checks in order to continue\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "assert '2.14' in tf.__version__, print(tf.__version__)\n",
        "assert '2.5.' in torch.__version__, print(torch.__version__)\n",
        "\n",
        "print(f'Is cuda available: {torch.cuda.is_available()}')\n",
        "assert torch.cuda.is_available(), \"GPU is required, for Colab see for example, https://colab.research.google.com/notebooks/gpu.ipynb\" # training requires 254M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-806OqutPEXA"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZpXlyZzPEXA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Known errors:\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1 which is incompatible.\n",
        "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1 which is incompatible.\n",
        "\"\"\"\n",
        "NANODET_COMMIT = 'pytorch2.0'\n",
        "!rm -rf nanodet\n",
        "!git clone https://github.com/RangiLyu/nanodet.git\n",
        "!touch nanodet/nanodet/model/__init__.py\n",
        "!cd nanodet && git checkout {NANODET_COMMIT} && pip install -q --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5UQm0tpPEXA"
      },
      "source": [
        "# Dataset\n",
        "- go to https://universe.roboflow.com/ai-camp-safety-equipment-detection/ppe-detection-using-cv/dataset/3 and click `\"Download Dataset\"`\n",
        "- choose format `\"COCO\"` and `\"show download code\"` and `\"continue\"`\n",
        "- choose `\"Terminal\"` and copy the command `\"curl...\"` and paste the command in the cell below.\n",
        "- add `\"!\"` in the beginning of the command and replace `\"\\&gt;\"` with `\">\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc2UelenPEXB"
      },
      "outputs": [],
      "source": [
        "# Add below your download code from Roboflow, it should look like the following, with your unique roboflow dataset url:\n",
        "# Example (with \"!\" added in the beginning of the command and replaced \"&gt;\" with \">\". Also added \"-q\" for less output):\n",
        "# !curl -L \"https://universe.roboflow.com/ds/<unique-dataset-url>\" > roboflow.zip; unzip -q roboflow.zip; rm roboflow.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1BiA2rXPEXB"
      },
      "outputs": [],
      "source": [
        "# Move test/train/valid to dataset folder\n",
        "from pathlib import Path\n",
        "DATASET_PATH = 'dataset/PPE_Detection_Using_CV.v3i.coco'\n",
        "if not Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists():\n",
        "    assert Path(f'train/_annotations.coco.json').exists()\n",
        "    assert Path(f'valid/_annotations.coco.json').exists()\n",
        "    assert Path(f'test/_annotations.coco.json').exists()\n",
        "    !mkdir -p $DATASET_PATH\n",
        "    !mv test train valid *txt $DATASET_PATH/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTagPSkEPEXC"
      },
      "outputs": [],
      "source": [
        "assert Path(f'{DATASET_PATH}/train/_annotations.coco.json').exists()\n",
        "assert Path(f'{DATASET_PATH}/valid/_annotations.coco.json').exists()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM2ZDbuCPEXC"
      },
      "source": [
        "# Training config file: nanodet-plus-m-1.5x_416-ppe.yml\n",
        "The following block of code creates the NanoDet training config file which\n",
        "is based on nanodet/config/nanodet-plus-m-1.5x_416.yml.\n",
        "Updated for the custom PPE dataset\n",
        "Change number of `total_epochs` for better performance.\n",
        "\n",
        "If training on more than 1 GPU, then set `gpu_ids`:\n",
        " * 2 gpu: [0,1]\n",
        " * etc...\n",
        "\n",
        "Increase `total_epochs`, for example 20.\n",
        "\n",
        "Feel free to increase `val_intervals`, for example 10.\n",
        "\n",
        "## Resume training\n",
        "Uncomment `resume` and `load_model` and add path to trained weights, For example: `workspace/nanodet-plus-m-1.5x_416-ppe/model_last.ckpt`\n",
        "\n",
        "For details see NanoDet github repo and [config docs](https://github.com/RangiLyu/nanodet/blob/main/docs/config_file_detail.md). Observe recommendation to adjust `lr` with `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ8NrfXgPEXD"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "touch nanodet-plus-m-1.5x_416-ppe.yml\n",
        "cat <<EOF >nanodet-plus-m-1.5x_416-ppe.yml\n",
        "# Comments:\n",
        "# -  based on nanodet/config/nanodet-plus-m-1.5x_416.yml\n",
        "# -  \"device\": settings for colab T4 GPU\n",
        "# -  \"total_epochs\": set to 20 during testing, default 300\n",
        "save_dir: workspace/nanodet-plus-m-1.5x_416-ppe\n",
        "model:\n",
        "  weight_averager:\n",
        "    name: ExpMovingAverager\n",
        "    decay: 0.9998\n",
        "  arch:\n",
        "    name: NanoDetPlus\n",
        "    detach_epoch: 10\n",
        "    backbone:\n",
        "      name: ShuffleNetV2\n",
        "      model_size: 1.5x\n",
        "      out_stages: [2,3,4]\n",
        "      activation: LeakyReLU\n",
        "    fpn:\n",
        "      name: GhostPAN\n",
        "      in_channels: [176, 352, 704]\n",
        "      out_channels: 128\n",
        "      kernel_size: 5\n",
        "      num_extra_level: 1\n",
        "      use_depthwise: True\n",
        "      activation: LeakyReLU\n",
        "    head:\n",
        "      name: NanoDetPlusHead\n",
        "      num_classes: 8\n",
        "      input_channel: 128\n",
        "      feat_channels: 128\n",
        "      stacked_convs: 2\n",
        "      kernel_size: 5\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "      norm_cfg:\n",
        "        type: BN\n",
        "      loss:\n",
        "        loss_qfl:\n",
        "          name: QualityFocalLoss\n",
        "          use_sigmoid: True\n",
        "          beta: 2.0\n",
        "          loss_weight: 1.0\n",
        "        loss_dfl:\n",
        "          name: DistributionFocalLoss\n",
        "          loss_weight: 0.25\n",
        "        loss_bbox:\n",
        "          name: GIoULoss\n",
        "          loss_weight: 2.0\n",
        "    # Auxiliary head, only use in training time.\n",
        "    aux_head:\n",
        "      name: SimpleConvHead\n",
        "      num_classes: 8\n",
        "      input_channel: 256\n",
        "      feat_channels: 256\n",
        "      stacked_convs: 4\n",
        "      strides: [8, 16, 32, 64]\n",
        "      activation: LeakyReLU\n",
        "      reg_max: 7\n",
        "data:\n",
        "  train:\n",
        "    name: CocoDataset\n",
        "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/train\n",
        "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json\n",
        "    input_size: [416,416] #[w,h]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      perspective: 0.0\n",
        "      scale: [0.6, 1.4]\n",
        "      stretch: [[0.8, 1.2], [0.8, 1.2]]\n",
        "      rotation: 0\n",
        "      shear: 0\n",
        "      translate: 0.2\n",
        "      flip: 0.5\n",
        "      brightness: 0.2\n",
        "      contrast: [0.6, 1.4]\n",
        "      saturation: [0.5, 1.2]\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "  val:\n",
        "    name: CocoDataset\n",
        "    img_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid\n",
        "    ann_path: dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json\n",
        "    input_size: [416,416] #[w,h]\n",
        "    keep_ratio: False\n",
        "    pipeline:\n",
        "      normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n",
        "device:\n",
        "  gpu_ids: [0]\n",
        "  workers_per_gpu: 2\n",
        "  batchsize_per_gpu: 32\n",
        "  precision: 32 # set to 16 to use AMP training\n",
        "schedule:\n",
        "#  resume:\n",
        "#  load_model:\n",
        "  optimizer:\n",
        "    name: AdamW\n",
        "    lr: 0.001\n",
        "    weight_decay: 0.05\n",
        "  warmup:\n",
        "    name: linear\n",
        "    steps: 500\n",
        "    ratio: 0.0001\n",
        "  total_epochs: 5\n",
        "  lr_schedule:\n",
        "    name: CosineAnnealingLR\n",
        "    T_max: 300\n",
        "    eta_min: 0.00005\n",
        "  val_intervals: 5\n",
        "grad_clip: 35\n",
        "evaluator:\n",
        "  name: CocoDetectionEvaluator\n",
        "  save_key: mAP\n",
        "log:\n",
        "  interval: 10\n",
        "\n",
        "class_names: [\n",
        "  'safety-equipment',\n",
        "  'person',\n",
        "  'goggles',\n",
        "  'helmet',\n",
        "  'no-goggles',\n",
        "  'no-helmet',\n",
        "  'no-vest',\n",
        "  'vest']\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeDjiLrSPEXD"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm1nI23rPEXD"
      },
      "outputs": [],
      "source": [
        "# OBSERVE: update the following assert statement to match your yml file settings.\n",
        "\n",
        "import yaml\n",
        "with open('nanodet-plus-m-1.5x_416-ppe.yml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "gpu_ids = config['device']['gpu_ids']\n",
        "assert isinstance(gpu_ids, list) and gpu_ids, print(f\"gpu_ids: {config['device']['gpu_ids']}\")\n",
        "assert config['schedule']['total_epochs'] == 5 or config['schedule']['total_epochs'] == 20, print(f\"total_epochs: {config['schedule']['total_epochs']}\")\n",
        "assert config['schedule']['val_intervals'] == 5 or config['schedule']['val_intervals'] == 10, print(f\"val_intervals: {config['schedule']['val_intervals']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HBZCGTXPEXE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "assert '2.5.' in torch.__version__, print(torch.__version__)\n",
        "assert Path('nanodet-plus-m-1.5x_416-ppe.yml').exists()\n",
        "!export PYTHONPATH=$PWD/nanodet:$PYTHONPATH && python nanodet/tools/train.py nanodet-plus-m-1.5x_416-ppe.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka2RJRtvPEXE"
      },
      "source": [
        "# Remove aux layers that are only used during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAD2HJioPEXE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"./nanodet\")\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "from nanodet.model.arch import build_model\n",
        "from nanodet.util import cfg, load_config, Logger\n",
        "\n",
        "def remove_aux(cfg, model_path, remove_layers=['aux_fpn', 'aux_head'], debug=False):\n",
        "    model = build_model(cfg.model)\n",
        "    ckpt = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "    if len(remove_layers) > 0:\n",
        "        state_dict = copy.deepcopy(ckpt['state_dict'])\n",
        "        for rlayer in remove_layers:\n",
        "            for layer in ckpt['state_dict']:\n",
        "                if rlayer in layer:\n",
        "                    del state_dict[layer]\n",
        "                    if debug:\n",
        "                        print(f'removed layer: {layer}')\n",
        "        del ckpt['state_dict']\n",
        "        ckpt['state_dict'] = copy.deepcopy(state_dict)\n",
        "        del state_dict\n",
        "    return ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDNDdD9-PEXE"
      },
      "outputs": [],
      "source": [
        "config_path = 'nanodet-plus-m-1.5x_416-ppe.yml'\n",
        "model_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best.pth'\n",
        "dst_path = 'workspace/nanodet-plus-m-1.5x_416-ppe/model_best/nanodet_model_best-removed-aux.pth'\n",
        "\n",
        "load_config(cfg, config_path)\n",
        "ckpt = remove_aux(cfg, model_path, ['aux_fpn', 'aux_head'])\n",
        "torch.save(ckpt, dst_path)\n",
        "print(f'Saved to: {dst_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpM_K93bPEXF"
      },
      "outputs": [],
      "source": [
        "# Compare size w and w/o aux\n",
        "!ls -l workspace/nanodet-plus-m-1.5x_416-ppe/model_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTM7Gie4PEXF"
      },
      "source": [
        "# Quantization of custom NanoDet model using Model Compression Toolkit\n",
        "Quantization is based on examples from [Model Compression Toolkit Keras Tutorials](https://github.com/sony/model_optimization/blob/v2.0.0/tutorials/notebooks/keras/ptq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZlbGU3LPEXF"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ablf72ePEXF"
      },
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir -q tensorflow~=2.14.0 pycocotools\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "if not importlib.util.find_spec('model_compression_toolkit'):\n",
        "    !pip install -q model_compression_toolkit==2.2.0\n",
        "!rm -rf temp_mct\n",
        "!git clone https://github.com/sony/model_optimization.git temp_mct && cd temp_mct && git checkout v2.2.0 && cd ..\n",
        "!mv temp_mct/tutorials . && rm -rf temp_mct\n",
        "sys.path.insert(0,\"tutorials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPFpiAIjPEXF"
      },
      "source": [
        "# Keras NanoDet float model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFCixxMpPEXF"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "assert '2.14' in tf.__version__, print(tf.__version__)\n",
        "assert '2.5.' in torch.__version__, print(torch.__version__)\n",
        "\n",
        "from keras.models import Model\n",
        "import model_compression_toolkit as mct\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_plus_m\n",
        "from tutorials.mct_model_garden.models_keras.utils.torch2keras_weights_translation import load_state_dict\n",
        "from tutorials.mct_model_garden.models_keras.nanodet.nanodet_keras_model import nanodet_box_decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bouupqxaPEXG"
      },
      "outputs": [],
      "source": [
        "# Upload the trained custom model\n",
        "CUSTOM_WEIGHTS_FILE = dst_path  # The NanoDet model trained with PPE dataset\n",
        "CLASS_NAMES = [\n",
        "  'safety-equipment',\n",
        "  'person',\n",
        "  'goggles',\n",
        "  'helmet',\n",
        "  'no-goggles',\n",
        "  'no-helmet',\n",
        "  'no-vest',\n",
        "  'vest']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "DATASET_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train'\n",
        "ANNOT_TRAIN = 'dataset/PPE_Detection_Using_CV.v3i.coco/train/_annotations.coco.json'\n",
        "DATASET_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid'\n",
        "ANNOT_VALID = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/_annotations.coco.json'\n",
        "DATASET_REPR = DATASET_VALID\n",
        "ANNOT_REPR = ANNOT_VALID\n",
        "\n",
        "QUANTIZED_MODEL = 'nanodet-quant-ppe.keras'\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "N_ITER = 20  # 1 for testing, otherwise 20\n",
        "\n",
        "assert Path(CUSTOM_WEIGHTS_FILE).exists()\n",
        "assert Path(DATASET_REPR).exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWJIbUhUPEXG"
      },
      "outputs": [],
      "source": [
        "def get_model(weights=CUSTOM_WEIGHTS_FILE, num_classes=NUM_CLASSES):\n",
        "    INPUT_RESOLUTION = 416\n",
        "    INPUT_SHAPE = (INPUT_RESOLUTION, INPUT_RESOLUTION, 3)\n",
        "    SCALE_FACTOR = 1.5\n",
        "    BOTTLENECK_RATIO = 0.5\n",
        "    FEATURE_CHANNELS = 128\n",
        "\n",
        "    pretrained_weights = torch.load(weights, map_location=torch.device('cpu'))['state_dict']\n",
        "    # Generate Nanodet base model\n",
        "    model = nanodet_plus_m(INPUT_SHAPE, SCALE_FACTOR, BOTTLENECK_RATIO, FEATURE_CHANNELS, num_classes)\n",
        "\n",
        "    # Set the pre-trained weights\n",
        "    load_state_dict(model, state_dict_torch=pretrained_weights)\n",
        "\n",
        "    # Add Nanodet Box decoding layer (decode the model outputs to bounding box coordinates)\n",
        "    scores, boxes = nanodet_box_decoding(model.output, res=INPUT_RESOLUTION, num_classes=num_classes)\n",
        "\n",
        "    # Add TensorFlow NMS layer\n",
        "    outputs = tf.image.combined_non_max_suppression(\n",
        "        boxes,\n",
        "        scores,\n",
        "        max_output_size_per_class=300,\n",
        "        max_total_size=300,\n",
        "        iou_threshold=0.65,\n",
        "        score_threshold=0.001,\n",
        "        pad_per_class=False,\n",
        "        clip_boxes=False\n",
        "        )\n",
        "\n",
        "    model = Model(model.input, outputs, name='Nanodet_plus_m_1.5x_416')\n",
        "\n",
        "    print('Model is ready for evaluation')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxHu-J4NPEXG"
      },
      "outputs": [],
      "source": [
        "# known warning:  WARNING: head.distribution_project.project not assigned to keras model !!!\n",
        "float_model = get_model(CUSTOM_WEIGHTS_FILE, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jpQCOvePEXG"
      },
      "source": [
        "# PTQ quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_TVXvTHPEXG"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Iterator, Tuple, List\n",
        "\n",
        "import cv2\n",
        "from tutorials.mct_model_garden.evaluation_metrics.coco_evaluation import coco_dataset_generator, CocoEval\n",
        "\n",
        "def nanodet_preprocess(x):\n",
        "    img_mean = [103.53, 116.28, 123.675]\n",
        "    img_std = [57.375, 57.12, 58.395]\n",
        "    x = cv2.resize(x, (416, 416))\n",
        "    x = (x - img_mean) / img_std\n",
        "    return x\n",
        "\n",
        "def get_representative_dataset(n_iter: int, dataset_loader: Iterator[Tuple]):\n",
        "    def representative_dataset() -> Iterator[List]:\n",
        "        ds_iter = iter(dataset_loader)\n",
        "        for _ in range(n_iter):\n",
        "            yield [next(ds_iter)[0]]\n",
        "\n",
        "    return representative_dataset\n",
        "\n",
        "def quantization(float_model, dataset, annot, n_iter=N_ITER):\n",
        "    # Load representative dataset\n",
        "    representative_dataset = coco_dataset_generator(dataset_folder=dataset,\n",
        "                                                    annotation_file=annot,\n",
        "                                                    preprocess=nanodet_preprocess,\n",
        "                                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "    tpc = mct.get_target_platform_capabilities('tensorflow', 'imx500')\n",
        "\n",
        "    # Preform post training quantization\n",
        "    quant_model, _ = mct.ptq.keras_post_training_quantization(\n",
        "        float_model,\n",
        "        representative_data_gen=get_representative_dataset(n_iter, representative_dataset),\n",
        "        target_platform_capabilities=tpc)\n",
        "\n",
        "    print('Quantized model is ready')\n",
        "    return quant_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC6bmbv6PEXH"
      },
      "outputs": [],
      "source": [
        "quant_model = quantization(float_model, DATASET_REPR, ANNOT_REPR)\n",
        "print(f'Representative dataset: {DATASET_REPR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBhUIHtMPEXH"
      },
      "outputs": [],
      "source": [
        "# Observe that loading quantized model might require specification of custom layers,\n",
        "# see https://github.com/sony/model_optimization/issues/1104\n",
        "mct.exporter.keras_export_model(model=quant_model, save_model_path=QUANTIZED_MODEL)\n",
        "print(f'Quantized model saved: {QUANTIZED_MODEL}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUeTjz-wPEXH"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from tutorials.mct_model_garden.evaluation_metrics.coco_evaluation import CocoDataset, model_predict, load_and_preprocess_image\n",
        "from tutorials.mct_model_garden.models_pytorch.yolov8.yolov8_postprocess import clip_boxes, clip_coords, scale_coords, scale_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lIaayo_PEXH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Helper function to perform coco evaluation with custom dataset\n",
        "\"\"\"\n",
        "from typing import List, Dict, Tuple, Callable\n",
        "import numpy as np\n",
        "\n",
        "# slow no batch version\n",
        "def format_results(outputs: List, img_ids: List, orig_img_dims: List, output_resize: Dict, custom_labels: Callable) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Format model outputs into a list of detection dictionaries.\n",
        "\n",
        "    Args:\n",
        "        outputs (list): List of model outputs, typically containing bounding boxes, scores, and labels.\n",
        "        img_ids (list): List of image IDs corresponding to each output.\n",
        "        orig_img_dims (list): List of tuples representing the original image dimensions (h, w) for each output.\n",
        "        output_resize (Dict): Contains the resize information to map between the model's\n",
        "                 output and the original image dimensions.\n",
        "        custom_labels (Callable): A function to map label outputs. Typically, COCO re-map from 80 (model) to 91 (dataset)\n",
        "\n",
        "    Returns:\n",
        "        list: A list of detection dictionaries, each containing information about the detected object.\n",
        "    \"\"\"\n",
        "    detections = []\n",
        "    h_model, w_model = output_resize['shape']\n",
        "    preserve_aspect_ratio = output_resize['aspect_ratio_preservation']\n",
        "\n",
        "    image_id = img_ids\n",
        "    scores = outputs[1].numpy().squeeze()  # Extract scores\n",
        "    labels = (custom_labels(outputs[2].numpy())).squeeze()  # Provide a function to map label outputs\n",
        "    boxes = outputs[0].numpy().squeeze()  # Extract bounding boxes\n",
        "    boxes = scale_boxes(boxes, orig_img_dims[0], orig_img_dims[1], h_model, w_model, preserve_aspect_ratio)\n",
        "    for score, label, box in zip(scores, labels, boxes):\n",
        "        if score == 0.0:\n",
        "            continue\n",
        "        detection = {\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": label,\n",
        "            \"bbox\": [box[1], box[0], box[3] - box[1], box[2] - box[0]],\n",
        "            \"score\": score\n",
        "        }\n",
        "        detections.append(detection)\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIr0wVc_PEXH"
      },
      "outputs": [],
      "source": [
        "custom_dataset = CocoDataset(dataset_folder=DATASET_VALID,\n",
        "                             annotation_file=ANNOT_VALID,\n",
        "                             preprocess=nanodet_preprocess)\n",
        "\n",
        "MODEL = float_model\n",
        "INPUT_RESOLUTION = 416\n",
        "\n",
        "output_resize = {'shape': (INPUT_RESOLUTION, INPUT_RESOLUTION), 'aspect_ratio_preservation': False}\n",
        "coco_predictions = []\n",
        "for idx, (im, anns) in enumerate(custom_dataset):\n",
        "    #if idx > 2:\n",
        "    #    continue\n",
        "    outputs = model_predict(MODEL, np.expand_dims(im, axis=0))  # 4 tensors: bbox, scores, classes, detections\n",
        "    detections = format_results(outputs, anns[0]['image_id'], anns[0]['orig_img_dims'], output_resize, lambda x: x)\n",
        "    coco_predictions.extend(detections)\n",
        "    if (idx + 1) % 50 == 0:\n",
        "        print(f'processed {(idx + 1)} images')\n",
        "\n",
        "print(len(coco_predictions))\n",
        "\n",
        "cocoGt=COCO(ANNOT_VALID)\n",
        "cocoDt=cocoGt.loadRes(coco_predictions)\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()\n",
        "\"\"\"\n",
        "epochs = 20\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.507\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.214\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.067\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.147\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.304\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.242\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.439\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.228\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.370\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.560\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZV9LJZ6PEXI"
      },
      "outputs": [],
      "source": [
        "custom_dataset = CocoDataset(dataset_folder=DATASET_VALID,\n",
        "                             annotation_file=ANNOT_VALID,\n",
        "                             preprocess=nanodet_preprocess)\n",
        "\n",
        "MODEL = quant_model\n",
        "INPUT_RESOLUTION = 416\n",
        "\n",
        "output_resize = {'shape': (INPUT_RESOLUTION, INPUT_RESOLUTION), 'aspect_ratio_preservation': False}\n",
        "coco_predictions = []\n",
        "for idx, (im, anns) in enumerate(custom_dataset):\n",
        "    #if idx > 2:\n",
        "    #    continue\n",
        "    outputs = model_predict(MODEL, np.expand_dims(im, axis=0))  # 4 tensors: bbox, scores, classes, detections\n",
        "    detections = format_results(outputs, anns[0]['image_id'], anns[0]['orig_img_dims'], output_resize, lambda x: x)\n",
        "    coco_predictions.extend(detections)\n",
        "    if (idx + 1) % 50 == 0:\n",
        "        print(f'processed {(idx + 1)} images')\n",
        "\n",
        "print(len(coco_predictions))\n",
        "\n",
        "cocoGt=COCO(ANNOT_VALID)\n",
        "cocoDt=cocoGt.loadRes(coco_predictions)\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,'bbox')\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()\n",
        "\"\"\"\n",
        "epochs = 20\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.495\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.206\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.071\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.138\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.295\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.237\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.427\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.475\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.238\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.361\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H5UXJtdPEXI"
      },
      "source": [
        "# Visualize detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-ImDuY4PEXI"
      },
      "outputs": [],
      "source": [
        "# Helper functions for visualization\n",
        "import numpy as np\n",
        "\n",
        "# draw a single bounding box onto a numpy array image\n",
        "def draw_bounding_box(img, annotation, scale, class_id, score):\n",
        "    row = scale[0]\n",
        "    col = scale[1]\n",
        "    x_min, y_min = int(annotation[1]*col), int(annotation[0]*row)\n",
        "    x_max, y_max = int(annotation[3]*col), int(annotation[2]*row)\n",
        "\n",
        "    color = (0,255,0)\n",
        "\n",
        "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
        "    text = f'{int(class_id)}: {score:.2f}'\n",
        "    cv2.putText(img, text, (x_min + 10, y_min + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "# draw all annotation bounding boxes on an image\n",
        "def annotate_image(img, output, scale, threshold=0.55):\n",
        "    b = output[0].numpy()[0]\n",
        "    s = output[1].numpy()[0]\n",
        "    c = output[2].numpy()[0]\n",
        "    for index, row in enumerate(b):\n",
        "        if s[index] > threshold:\n",
        "            #print(f'row: {row}')\n",
        "            id = int(c[index])\n",
        "            draw_bounding_box(img, row, scale, id, s[index])\n",
        "            print(f'class: {CLASS_NAMES[id]} ({id}), score: {s[index]:.2f}')\n",
        "    return {'bbox':b, 'score':s, 'classes':c}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f27ikQWPEXI"
      },
      "outputs": [],
      "source": [
        "# See appendix for results. For 2 epochs, the bounding boxes are not perfect...\n",
        "# But improves considerably for 20 epochs.\n",
        "\n",
        "MODEL = quant_model\n",
        "\n",
        "test_img = 'dataset/PPE_Detection_Using_CV.v3i.coco/valid/image_257_jpg.rf.1a3a6eb456134cce302712c109645c26.jpg'\n",
        "img = load_and_preprocess_image(f'{test_img}', nanodet_preprocess)\n",
        "output = MODEL(np.expand_dims(img, axis=0))\n",
        "image = cv2.imread(f'{test_img}')\n",
        "print(f'image shape: {image.shape}')\n",
        "r = annotate_image(image, output, scale=image.shape)\n",
        "assert r['score'][0] > 0.5, print(f\"r['score'][0] > 0.5 failed: {r['score'][0]}\")\n",
        "dst = f'annotated.jpg'\n",
        "if cv2.imwrite(dst, image):\n",
        "    print(f'Annotated image saved to: {dst}')\n",
        "else:\n",
        "    print(f'Failed saving annotated image')\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "603Ymkt4PEXI"
      },
      "source": [
        "# Conversion\n",
        "For details see\n",
        "* [Raspberry Pi Documentation](https://www.raspberrypi.com/documentation/accessories/ai-camera.html#conversion)\n",
        "* [Sony IMX500 Converter documentation](https://developer.aitrios.sony-semicon.com/en/raspberrypi-ai-camera/documentation/imx500-converter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J31kEpdvPEXO"
      },
      "outputs": [],
      "source": [
        "!imxconv-tf -i {QUANTIZED_MODEL} -o converted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6KC0WelPEXO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Expected output from converter:\n",
        "dnnParams.xml\t\t nanodet-quant-ppe_MemoryReport.json\n",
        "nanodet-quant-ppe.pbtxt  packerOut.zip\n",
        "\"\"\"\n",
        "!ls converted\n",
        "assert os.path.exists(\"converted/packerOut.zip\"), f\"Converted file not found\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPNGMfBpPEXO"
      },
      "source": [
        "# Next step\n",
        "__OBSERVE__: First, save the quantized model and the output from the conversion to your local machine. For packaging you will need the `packerOut.zip` file.\n",
        "\n",
        "Next step is to package the model for IMX500, see [Raspberry Pi Documentation](https://www.raspberrypi.com/documentation/accessories/ai-camera.html#packaging)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}